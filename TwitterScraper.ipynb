{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T13:39:38.554642Z",
     "start_time": "2019-04-29T13:39:37.365184Z"
    }
   },
   "outputs": [],
   "source": [
    "from twitterscraper import query_tweets\n",
    "import codecs, json\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T15:18:37.414580Z",
     "start_time": "2019-04-18T15:18:37.410453Z"
    }
   },
   "source": [
    "## Verify File for Overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T13:39:38.632767Z",
     "start_time": "2019-04-29T13:39:38.617502Z"
    }
   },
   "outputs": [],
   "source": [
    "#Deletes previous JSON - Single Options\n",
    "def single_file_delete(path, filename):\n",
    "    total_path = str(path) + str(filename)\n",
    "    if os.path.isfile(total_path):\n",
    "        subprocess.call(['rm', str(total_path)]) #Call Remove call\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "#Single Delete test on \"tweets.json of the same directory\"\n",
    "# single_file_delete(\"./\", \"tweets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T13:39:38.912765Z",
     "start_time": "2019-04-29T13:39:38.906688Z"
    }
   },
   "outputs": [],
   "source": [
    "#Multiple Delete - Feed it a list of files to delete\n",
    "def multiple_file_delete(path, files):\n",
    "    for i in files:\n",
    "        single_file_delete(path, i)\n",
    "\n",
    "# Multiple Delete test on list containing 'tweets1.json', 'tweets2.json', 'tweets4.json'\n",
    "# for_delete = ['tweets1.json', 'tweets2.json', 'tweets4.json']\n",
    "# multiple_file_delete(\"./\", for_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T15:18:50.236486Z",
     "start_time": "2019-04-18T15:18:50.232259Z"
    }
   },
   "source": [
    "## Query Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T13:39:40.058001Z",
     "start_time": "2019-04-29T13:39:40.052373Z"
    }
   },
   "outputs": [],
   "source": [
    "#test_query = \"hurricane, OR flood, OR flooding, OR sandy near:40.605005,73.748324,10km since:2012-10-28 until:2012-10-31\"\n",
    "\n",
    "#List of possible search values\n",
    "predictors_list = ['hurricane', 'flood', 'flooding', 'sandy'] \n",
    "\n",
    "#joins query list together with OR\n",
    "final_predictors_list = \", OR \".join(predictors_list) \n",
    "\n",
    "#GPS Latitutde/Longitude\n",
    "latitude = 40.605005\n",
    "longitude = 73.748324\n",
    "\n",
    "#Radius of query\n",
    "radius = 7.5\n",
    "\n",
    "#Radius units\n",
    "radius_units = \"km\"\n",
    "\n",
    "#Begin date in YYYY-MM-DD format\n",
    "time_begin = '2012-10-28' \n",
    "\n",
    "#End date in YYYY-MM-DD format\n",
    "time_end = '2012-10-31' \n",
    "\n",
    "#Maximum amount of queries to grab\n",
    "max_query = 1000\n",
    "\n",
    "#Name of file to output\n",
    "file_name = 'tweets.json' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T13:39:41.381849Z",
     "start_time": "2019-04-29T13:39:41.375677Z"
    }
   },
   "outputs": [],
   "source": [
    "# final_assembled_query = str(final_predictors_list) + \" near:\" + str(latitude) + \",\" + str(longitude) + \",\" + str(radius) + radius_units + \" since:\" + time_begin + \" until:\" + time_end  \n",
    "    \n",
    "# test_query = \"hurricane, OR flood, OR flooding, OR sandy near:40.605005,73.748324,10km since:2012-10-28 until:2012-10-31\"\n",
    "# if final_assembled_query == test_query:\n",
    "#     print('yes')\n",
    "# print(final_assembled_query)\n",
    "# print(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T13:39:41.571472Z",
     "start_time": "2019-04-29T13:39:41.561652Z"
    }
   },
   "outputs": [],
   "source": [
    "#Feed list of gps lat/longs\n",
    "def twitter_grid_search(gps_coords):\n",
    "    count = 0\n",
    "    for i in gps_coords:\n",
    "        final_assembled_query = str(final_predictors_list) + \" near:\" + str(i[0]) + \",\" + str(i[1]) + \",\" + str(radius) + radius_units\n",
    "        \n",
    "        print(final_assembled_query)\n",
    "        \n",
    "        subprocess.call(['twitterscraper', final_assembled_query, \"-bd\", time_begin, \"-ed\", time_end, \"-o\", \"./data/tweets\" + str(count) + \".json\", \"-l\", str(max_query)])\n",
    "        count += 1\n",
    "        \n",
    "# final_assembled_query = str(final_predictors_list) + \" near:\" + str(i[0]) + \",\" + str(i[1]) + \",\" + str(radius) + radius_units + \" since:\" + time_begin + \" until:\" + time_end  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:01:29.713237Z",
     "start_time": "2019-04-19T21:01:29.697949Z"
    }
   },
   "source": [
    "## Test Code ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T13:39:44.199269Z",
     "start_time": "2019-04-29T13:39:44.195515Z"
    }
   },
   "outputs": [],
   "source": [
    "# x = [5,10,20,30]\n",
    "# count = 0\n",
    "# for i in x:\n",
    "#     subprocess.call(['twitterscraper', final_assembled_query, \"-o\", \"tweets\" + str(count) + \".json\", \"-l\", str(i)])\n",
    "#     count += 1\n",
    "# !twitterscraper \"hurricane, OR flood, OR flooding, OR sandy near:40.605005,73.748324,10km since:2012-10-28 until:2012-10-31\" -o tweets.json -l 10000;\n",
    "# final_assembled_query\n",
    "# !twitterscraper \"$final_assembled_query\" -o $tweets.json -l 10000;\n",
    "#!twitterscraper \"hurricane, OR flood, OR flooding, OR sandy near:40.605005,73.748324,10km since:2012-10-28 until:2012-10-31\" -o tweets.json -l 10000;\n",
    "# !twitterscraper \"$final_assembled_query\" -o $tweets.json -l 10000;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Grid Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T13:40:33.165468Z",
     "start_time": "2019-04-29T13:40:33.142082Z"
    }
   },
   "outputs": [],
   "source": [
    "test_grid = [(40.527428, -76.719904),\n",
    " (40.66217607430053, -76.719904),\n",
    " (40.79692414860107, -76.719904),\n",
    " (40.931672222901604, -76.719904),\n",
    " (40.52729204484159, -76.54262629794702),\n",
    " (40.66204011914212, -76.54262629794702),\n",
    " (40.79678819344265, -76.54262629794702),\n",
    " (40.93153626774318, -76.54262629794702),\n",
    " (40.527156090333946, -76.36534895551316),\n",
    " (40.66190416463448, -76.36534895551317),\n",
    " (40.79665223893502, -76.36534895551317),\n",
    " (40.93140031323556, -76.36534895551317),\n",
    " (40.52702013647707, -76.18807197269423),\n",
    " (40.6617682107776, -76.18807197269423),\n",
    " (40.796516285078134, -76.18807197269423),\n",
    " (40.93126435937867, -76.18807197269423),\n",
    " (40.526884183270944, -76.01079534948605),\n",
    " (40.66163225757148, -76.01079534948605),\n",
    " (40.796380331872015, -76.01079534948605),\n",
    " (40.93112840617255, -76.01079534948605),\n",
    " (40.52674823071558, -75.83351908588443),\n",
    " (40.661496305016115, -75.83351908588443),\n",
    " (40.79624437931665, -75.83351908588443),\n",
    " (40.93099245361718, -75.83351908588443),\n",
    " (40.52661227881096, -75.65624318188522),\n",
    " (40.66136035311149, -75.65624318188522),\n",
    " (40.79610842741204, -75.65624318188522),\n",
    " (40.93085650171257, -75.65624318188522),\n",
    " (40.52647632755708, -75.47896763748422),\n",
    " (40.66122440185761, -75.47896763748422),\n",
    " (40.79597247615814, -75.47896763748422),\n",
    " (40.93072055045868, -75.47896763748422),\n",
    " (40.52634037695393, -75.30169245267726),\n",
    " (40.66108845125447, -75.30169245267726),\n",
    " (40.795836525555, -75.30169245267726),\n",
    " (40.93058459985554, -75.30169245267726),\n",
    " (40.52620442700153, -75.12441762746016),\n",
    " (40.66095250130207, -75.12441762746016),\n",
    " (40.795700575602616, -75.12441762746016),\n",
    " (40.93044864990315, -75.12441762746016),\n",
    " (40.52606847769985, -74.94714316182875),\n",
    " (40.660816552000384, -74.94714316182875),\n",
    " (40.795564626300916, -74.94714316182875),\n",
    " (40.93031270060145, -74.94714316182875),\n",
    " (40.52593252904889, -74.76986905577884),\n",
    " (40.66068060334943, -74.76986905577884),\n",
    " (40.795428677649966, -74.76986905577884),\n",
    " (40.9301767519505, -74.76986905577884),\n",
    " (40.52579658104864, -74.59259530930628),\n",
    " (40.66054465534918, -74.59259530930628),\n",
    " (40.79529272964971, -74.59259530930628),\n",
    " (40.93004080395025, -74.59259530930628),\n",
    " (40.5256606336991, -74.41532192240685),\n",
    " (40.66040870799963, -74.41532192240685),\n",
    " (40.79515678230016, -74.41532192240685),\n",
    " (40.9299048566007, -74.41532192240685),\n",
    " (40.52552468700025, -74.23804889507639),\n",
    " (40.66027276130079, -74.23804889507639),\n",
    " (40.79502083560131, -74.23804889507639),\n",
    " (40.92976890990185, -74.23804889507639)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T13:40:34.392194Z",
     "start_time": "2019-04-29T13:40:34.378211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T13:40:34.583696Z",
     "start_time": "2019-04-29T13:40:34.576460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.527428, -76.719904)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_grid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T13:40:34.800072Z",
     "start_time": "2019-04-29T13:40:34.784395Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_grid(gps_coords):\n",
    "    dict = {}\n",
    "    for i in range(len(gps_coords)):\n",
    "        with codecs.open('./data/tweets' + str(i) + '.json', 'r', 'utf-8') as f:\n",
    "            tweets = json.load(f, encoding='utf-8')\n",
    "\n",
    "        list_tweets = [list(elem.values()) for elem in tweets]\n",
    "        list_columns = list(tweets[0].keys())\n",
    "        dict['df'+ str(i)] = pd.DataFrame(list_tweets, columns=list_columns)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T13:40:45.900Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hurricane, OR flood, OR flooding, OR sandy near:40.527428,-76.719904,7.5km\n",
      "hurricane, OR flood, OR flooding, OR sandy near:40.66217607430053,-76.719904,7.5km\n",
      "hurricane, OR flood, OR flooding, OR sandy near:40.79692414860107,-76.719904,7.5km\n"
     ]
    }
   ],
   "source": [
    "twitter_grid_search(test_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T13:40:46.059Z"
    }
   },
   "outputs": [],
   "source": [
    "points = generate_grid(test_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T13:40:46.235Z"
    }
   },
   "outputs": [],
   "source": [
    "#Verify dimensions\n",
    "points['df0'].shape, points['df1'].shape, points['df2'].shape, points['df4'].shape, points['df4'].shape, points['df5'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T13:40:47.200Z"
    }
   },
   "outputs": [],
   "source": [
    "#Adds GPS coordinates to dataframes\n",
    "def add_gps_df(gps_coords, points):\n",
    "    count = 0\n",
    "    for i in points.keys():\n",
    "        points[i]['latitude'] = gps_coords[count][0]\n",
    "        points[i]['longitude'] = gps_coords[count][1]\n",
    "        count += 1\n",
    "        \n",
    "add_gps_df(test_grid, points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T13:40:47.365Z"
    }
   },
   "outputs": [],
   "source": [
    "points['df0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T14:55:11.463245Z",
     "start_time": "2019-04-18T14:55:11.430037Z"
    }
   },
   "source": [
    "## Convert pulled .json to data dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T13:40:48.659Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with codecs.open('tweets.json', 'r', 'utf-8') as f:\n",
    "    tweets = json.load(f, encoding='utf-8')\n",
    "\n",
    "list_tweets = [list(elem.values()) for elem in tweets]\n",
    "list_columns = list(tweets[0].keys())\n",
    "df = pd.DataFrame(list_tweets, columns=list_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-29T13:40:48.812Z"
    }
   },
   "outputs": [],
   "source": [
    "df['timestamp'].value_counts();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
